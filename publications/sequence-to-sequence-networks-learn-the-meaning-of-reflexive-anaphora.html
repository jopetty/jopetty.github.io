<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="Racket + Pollen + Colophon">
    <title>Jackson Petty</title>
    <link href="/static/css/fonts.css" rel="stylesheet">
    <link href="/static/css/style.css" rel="stylesheet">
  </head>
  <body><header><nav><span><a href="/" class="breadcrumb-link">Jackson Petty</a></span><span class="breadcrumb-spacer">/</span><span><a href="/publications/" class="breadcrumb-link">Publications</a></span></nav></header><div id="content"><article><h1>Sequence-to-Sequence Networks Learn the Meaning of Reflexive Anaphora</h1><div class="abstract" id="abstract"><p>Reflexive anaphora present a challenge for semantic interpretation: their meaning is not constant but must be derived from the context in a way that appears to require abstract variables. Past work has raised doubts about the ability of recurrent networks to meet this challenge. Using sequence-to-sequence architecture, this paper shows that recurrent networks are in fact capable of learning a lexically abstract semantic interpretation for reflexive anaphora that generalizes to novel antecedents. We explore the effect of attention mechanisms and different recurrent unit types on the type of training data that is needed for success: how much lexical support is needed to learn an abstract reflexive meaning and what contexts must an antecedent occur in to support generalization?</p></div></article></div></body>
</html>