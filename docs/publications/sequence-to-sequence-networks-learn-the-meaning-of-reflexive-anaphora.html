<!DOCTYPE html>
<html lang="en">
  <head><meta charset="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="generator" content="Racket + Pollen + Colophon"/><title>Sequence-to-Sequence Networks Learn the Meaning of Reflexive Anaphora</title><link href="/static/css/fonts.css" rel="stylesheet"/><link href="/static/css/style.css" rel="stylesheet"/><link href="/static/css/index.css" rel="stylesheet"/></head>
  <body><header><nav><span><a href="/" class="breadcrumb-link">Jackson Petty</a></span><span class="breadcrumb-spacer">/</span><span><a href="/publications/" class="breadcrumb-link">Publications</a></span></nav></header><div id="content"><article><h1>Sequence-to-Sequence Networks Learn the Meaning of Reflexive Anaphora</h1><h3>Robert Frank and Jackson Petty</h3><div class="abstract" id="abstract"><p>Reflexive anaphora present a challenge for semantic interpretation: their meaning varies depending on context in a way that appears to require abstract variables. Past work has raised doubts about the ability of recurrent networks to meet this challenge. In this paper, we explore this question in the context of a fragment of English that incorporates the relevant sort of contextual variability. We consider sequence-to-sequence architectures with recurrent units and show that such networks are capable of learning semantic interpretations for reflexive anaphora which generalize to novel antecedents. We explore the effect of attention mechanisms and different recurrent unit types on the type of training data that is needed for success as measured in two ways: how much lexical support is needed to induce an abstract reflexive meaning (i.e., how many distinct reflexive antecedents must occur during training) and what contexts must a noun phrase occur in to support generalization of reflexive interpretation to this noun phrase?</p></div><h2>Introduction</h2><p>Recurrent neural network architectures have demonstrated remarkable success in natural language processing, achieving state of the art performance across an impressive range of tasks ranging from machine translation to semantic parsing to question answering (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2016). These tasks demand the use of a wide variety of computational processes and information sources (from grammatical to lexical to world knowledge), and are evaluated in coarse-grained quantitative ways. As a result, it is not an easy matter to identify the specific strengths and weaknesses in a network’s solution of a task.</p><p>In this paper, we take a different tack, exploring the degree to which neural networks successfully master one very specific aspect of linguistic knowledge: the interpretation of sentences containing reflexive anaphora. We address this problem in the context of the task of semantic parsing, which we instantiate as mapping a sequence of words into a predicate calculus logical form representation of the sentence’s meaning.</p></article></div></body>
</html>